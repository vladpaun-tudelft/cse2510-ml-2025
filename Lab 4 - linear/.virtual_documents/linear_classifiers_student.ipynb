











import scipy
import sklearn
import numpy as np
import matplotlib.pyplot as plt





from sklearn import datasets

# Load the digits with 10 classes (digits 0 - 9)
all_digits = datasets.load_digits(n_class=10)
all_digits_images = all_digits.images
all_digits_labels = all_digits.target

'''
all_digits_images is a numpy array where:
- the first index is the index of individual images
- the second index corresponds to the row of the pixel
- the third index corresponds to the column of the pixel
i.e.: all_digits_images[image_index,row,column]
the values of the pixels are values between 0 (black) and 16 (white)
'''

for i in range(10):
    digit_image = all_digits_images[i,:,:]
    plt.figure()
    plt.gray()
    plt.title("digit: " + str(all_digits_labels[i]))
    plt.imshow(digit_image)





# Load the digits with 2 classes (0 and 1)
binary_digits = datasets.load_digits(n_class=2)
binary_digits_images = binary_digits.images
binary_digits_labels = binary_digits.target

for i in range(10):
    digit_image = binary_digits_images[i,:,:]
    plt.figure()
    plt.gray()
    plt.title("digit: " + str(binary_digits_labels[i]))
    plt.imshow(digit_image)





# width: average of the column-wise max values
widths = np.zeros(len(binary_digits_images))
# START ANSWER
widths = np.mean(np.max(binary_digits_images, axis=1), axis=1)
# END ANSWER

# length: average of the row-wise max values
lengths = np.zeros(len(binary_digits_images))
# START ANSWER
lengths = np.mean(np.max(binary_digits_images, axis=2), axis=1)
# END ANSWER

assert (widths[:5] == np.array([8.5, 8.0, 9.25, 8.125, 9.5])).all()
assert (lengths[:5] == np.array([12.875, 15.625, 15.0, 15.75, 14.875])).all()





binary_digits_features = np.vstack((widths, lengths)).T
print(binary_digits_features[:10])





def plot_scatter(features, labels, db_x = None, db_y = None):
    widths = features[:,0]
    lengths = features[:,1]
    
    # Separate the 2 classes
    widths_0 = widths[labels == 0]
    lengths_0 = lengths[labels == 0]
    widths_1 = widths[labels == 1]
    lengths_1 = lengths[labels == 1]

    # Plot
    plt.scatter(widths_1, lengths_1, c='blue', label='ones')
    plt.scatter(widths_0, lengths_0, c='red', label='zeros')
    
    # Extra code to plot the decision boundary
    # You won't be using this right away
    if not(db_x is None or db_y is None):
        plt.plot(db_x, db_y, label = "Decision_Boundary")

    plt.title('Digits')
    plt.xlabel('width')
    plt.ylabel('length')
    plt.axis('square')
    plt.xticks(np.arange(widths.min(), widths.max()+2, 1, dtype=int))
    plt.yticks(np.arange(lengths.min()-1, lengths.max()+2, 1, dtype=int))
    plt.xlim((widths.min()-1, widths.max()+1))
    plt.ylim((lengths.min()-1, lengths.max()+1))
    plt.legend(loc=3)
    plt.show()



plot_scatter(binary_digits_features, binary_digits_labels)














def plot_hypothesis(features, labels, theta, bias):
    # Some noise is added to better visualize the labels of the datapoints
    labels_noise = labels + np.random.normal(0, .05, labels.shape)
    
    widths = features[:,0]
    plt.scatter(widths, labels_noise, c='red')
    x = np.linspace(np.min(widths), np.max(widths), 100)
        
    sigmoid_1D = 1 / (1 + np.exp(-(theta*x + bias)))
    plt.title('hypothesis function')
    plt.xlabel('x')
    plt.ylabel('label (red)/probability class 1 (blue)')
    plt.plot(x, sigmoid_1D)
    plt.show()

# Try to find proper values for theta and bias
# Such that the sigmoid properly goes through both the datapoints with label 0 and 1
theta = -10
bias = 100
# START ANSWER
# END ANSWER

plot_hypothesis(binary_digits_features, binary_digits_labels, theta, bias)








dummy_data = np.array([[4,1], [8,10]])
dummy_labels = np.array([0, 1])

# Try to find proper values for theta and bias
# such that the sigmoid properly goes through both the datapoints with label 0 and 1. 
# Note that the points are slightly jittered: they should have value 0 and 1 respectively
theta = 100
bias = -600
# START ANSWER
# END ANSWER

plot_hypothesis(dummy_data, dummy_labels, theta, bias)





# This function adds an extra 1.0 to every feature vector
def add_one_features(data):
    return np.vstack((data.T, np.ones(len(data)))).T

binary_digits_features_prime = add_one_features(binary_digits_features)
print(binary_digits_features_prime[:10])





# Implement the hypothesis function so that it works for thetas/features of arbitrary length
def hypothesis(x, theta):
    """
    Calculate the hypothesis function for every datapoint in x
    :param x: numpy array of size (n, d) where n is the number of samples
    and d is the number of features per sample including the 1 extra feature
    :param theta: numpy array of size (d,)
    :return: predicted probability.
    """
    # START ANSWER
    Z = x @ theta
    sigmoid = 1 / (1 + np.exp(-Z))
    # END ANSWER
    return sigmoid

x = binary_digits_features_prime
# To test our hypothesis function, we set three different theta vectors
# All 1
theta_ones = np.ones(3)
# All 0
theta_zeros = np.zeros(3)
# All -1
theta_min_ones = -5 * np.ones(3)

# And apply the prediction
hypothesis_ones = hypothesis(x, theta_ones)
hypothesis_zeros = hypothesis(x, theta_zeros)
hypothesis_min_fives = hypothesis(x, theta_min_ones)

# Output for each theta vector
# expected = 1.0
print("Prediction ones: {}".format(hypothesis_ones[:5]))
# expected = 0.5
print("Prediction zeros: {}".format(hypothesis_zeros[:5]))
# expected = ~0
print("Prediction minus fives: {}".format(hypothesis_min_fives[:5]))

assert np.isclose(hypothesis_ones, 1).all()
assert np.isclose(hypothesis_zeros, 0.5).all()
assert np.isclose(hypothesis_min_fives, 0).all()








near_0 = 1e-16
near_1 = 1.0 - near_0

def log_likelihood(h_x, y):
    """
    Computes the log likelihood of your classifier.
    :param h_x: numpy array of predicted probabilities.
    :param y: numpy array of actual labels (positive (1) or negative (0)).
    :return: The log likelihood.
    """
    log_likelihood = 0
    # START ANSWER
    h_x = np.where(h_x == 0, near_0, h_x)
    h_x = np.where(h_x == 1, near_1, h_x)
    log_likelihood = np.sum(y * np.log(h_x) + (1 - y) * np.log(1 - h_x))
    # END ANSWER
    return log_likelihood

# These predictions should do very well
h_x1 = np.array([0.01, 0.01, 0.99, 0.99])
y1 = np.array([0, 0, 1, 1])
ll1 = log_likelihood(h_x1, y1)
print(ll1)

# These predictions should do ok
h_x2 = np.array([0.2, 0.1, 0.9, 0.8])
y2 = np.array([0, 0, 1, 1])
ll2 = log_likelihood(h_x2, y2)
print(ll2)

# These predictions should do bad
h_x3 = np.array([0.9, 0.8, 0.99, 0.3, 0.1])
y3 = np.array([0, 0, 1, 1, 1])
ll3 = log_likelihood(h_x3, y3)
print(ll3)

assert np.isclose(ll1, -0.040201)
assert np.isclose(ll2, -0.657008)
assert np.isclose(ll3, -7.428631)

# There might be warnings from numpy regarding division by zero and invalid value. 
# You can solve this by replacing 0/1 values with near_0/near_1 values with the np.where function
h_x4 = np.array([0.0, 0.1, 1.0, 0.95])
y4 = np.array([0, 0, 1, 1])
ll4 = log_likelihood(h_x4, y4)
print(ll4)
# h_x5 0.0 will become 1e-16 so you can divide with that.
h_x5 = np.array([1.0, 0.99, 0.0, 0.01])
y5 = np.array([0, 0, 1, 1])
ll5 = log_likelihood(h_x5, y5)
print(ll5)

assert np.isclose(ll4, -0.156653, rtol=0.5)
# Due to the wrong predictions, this likelihood is very low
assert ll5 < -10.0





# Use only the width feature
width_features = binary_digits_features[:,0]
width_features_prime = add_one_features(width_features)
binary_digits_labels

# Axis limits to plot
min_theta_0 = -20.0
max_theta_0 = 5.0
min_bias = -30.0
max_bias = 150.0

# Resolution for both axis
N = 50
thetas_0 = np.linspace(min_theta_0, max_theta_0, N)
biases = np.linspace(min_bias, max_bias, N)

# 2D array with log likelihoods to be filled
log_likelihoods = np.zeros(shape=(len(biases), len(thetas_0)))
# Fill the 2D array
for i_theta_0, theta_0 in enumerate(thetas_0):
    for i_bias, bias in enumerate(biases):
        # Construct theta
        theta = np.array([theta_0, bias])
        h_x = hypothesis(width_features_prime, theta)
        ll = log_likelihood(h_x, binary_digits_labels)
        log_likelihoods[i_bias,i_theta_0] = ll

# Plot log likelihoods
X, Y = np.meshgrid(thetas_0, biases)
cs = plt.contourf(X, Y, log_likelihoods, cmap="PuRd")
plt.title('Log-Likelihoods')
plt.xlabel(r'$\theta_0$')
plt.ylabel('bias')
plt.colorbar(cs)

plt.show()








def calculate_gradients(theta, x, y):
    """
    Calculate the gradient for every datapoint in x
    :param theta: numpy array of theta
    :param x: numpy array of the features
    :param y: the label (positive (1) or negative (0))
    :return: The gradients for every datapoint in x
    """
    gradients = np.zeros((len(x), len(theta)))
    # START ANSWER
    h_x = hypothesis(x, theta)
    gradients = (y - h_x)[:, None] * x

    # END ANSWER
    return gradients

theta = np.array([1,1.5,2.5])
x = np.array([[-10,5,1],[0.5,1,1]])
y = np.array([0,1])
gradients = calculate_gradients(theta, x, y)
print(gradients)

assert np.isclose(gradients[0], np.array([5.0, -2.5, -0.5])).all()
assert np.isclose(gradients[1], np.array([0.00549347, 0.01098694, 0.01098694]), atol= 0.0001).all()


def apply_gradient(theta, gradient, alpha):
    """
    Applies the gradient step to theta and returns an adjusted theta.
    :param theta: current theta array of size (d,)
    :param gradient: the gradient array of (d,)
    :param alpha: learning rate
    :return: the updated theta array of size (d,)
    """
    updated_theta = theta
    # START ANSWER
    updated_theta = theta + alpha * gradient
    # END ANSWER
    return updated_theta

theta = np.array([1,2,3])
gradient = np.array([10,-10,5])
alpha = 0.1
updated_theta = apply_gradient(theta, gradient, alpha)
print(updated_theta)

assert (updated_theta == np.array([2,1,3.5])).all()





def train_theta(features, labels, n_epochs=200, theta=None, alpha = 0.1):
    assert len(features) == len(labels)

    num_features = len(features[0])
    num_items = len(features)
    # Set theta to intial random values
    # Initialize theta randomly if it's not provided
    if theta is None:
        theta = np.random.normal(0, .05, num_features)

    # We go through the entire training set a number of times
    # Each of these iterations is called an epoch
    for epoch in range(n_epochs):
        # Calculate the average gradient for all items and apply gradient ascent to theta
        # START ANSWER
        h = hypothesis(features, theta)
        grads = (labels - h)[:,None] * features
        theta = theta + alpha * grads.mean(axis=0)
        # END ANSWER
    
    return theta


# Train a theta vector for the features and labels of the binary digits:
theta = train_theta(binary_digits_features_prime, binary_digits_labels, n_epochs=100000, alpha = 0.05)

print("theta vector:  " + str(theta))
print("log likelihood: " + str(log_likelihood(hypothesis(binary_digits_features_prime, theta),binary_digits_labels)))





def decision_boundary(theta, plot_x):
    return (-1/theta[1]) * (theta[0] * plot_x + theta[2])

def plot_decision_boundary(theta, data, labels):
    db_x = np.array([data[:,0].min()-1, data[:,0].max()+1])
    db_y = decision_boundary(theta, db_x)
    plot_scatter(data, labels, db_x = db_x, db_y = db_y)

plot_decision_boundary(theta, binary_digits_features_prime, binary_digits_labels)








from sklearn.model_selection import train_test_split

# Flatten the data so all items are 1D and append an extra one feature to every item
binary_digits_pixels = add_one_features(binary_digits_images.reshape(binary_digits_images.shape[0], -1))

# The shape should be (360, 65)
assert binary_digits_pixels.shape[0] == 360
assert binary_digits_pixels.shape[1] == 65

# Split dataset into train and test set
x_train_digits, x_test_digits, y_train_digits, y_test_digits = train_test_split(binary_digits_pixels, binary_digits_labels, test_size=0.3)





# train a theta vector for the features and labels of the binary digits:
theta_digits = train_theta(x_train_digits, y_train_digits, n_epochs=100000)
print("theta vector: " + str(theta_digits))





def predict_binary(x_test, theta):
    """
    Predicts a label for each image in x_test using theta.
    :param x_test: an array of size (n, 65) of all test images.
    :param theta: a (65,) array of trained theta.
    :return: an integer array of size (n,) of labels for each test_image.
    """
    predictions = np.zeros(x_test.shape[0], dtype=int)
    # START ANSWER
    z = x_test @ theta
    h = 1 / (1+np.exp(-z))
    predictions = np.where(h > 0.5, 1, 0)
    # END ANSWER
    return predictions

x_test = np.array([[1,2,3,1], [-1,2,1.5,1], [4,-5,2,4]])
theta = np.array([1,-1,2,-2])
predictions = predict_binary(x_test, theta)
print(predictions)

assert (predictions == np.array([1, 0, 1])).all()
assert predictions.dtype == np.dtype('int')





def compute_accuracy(predictions, y_true):
    """
    Computes the accuracy of the predictions based on the true labels.
    :param predictions: an array of size (n,) of the computed predictions for each image.
    :param y_true: an array of size (n,) of the true labels of each image.
    :return: the accuracy of the predictions.
    """
    accuracy = -1
    # START ANSWER
    accuracy = np.mean(predictions == y_true)
    # END ANSWER
    return accuracy

predictions = np.array([0,1,1,0,1])
y_true = np.array([0,1,0,1,1])

accuracy = compute_accuracy(predictions, y_true)
assert accuracy == 0.6





predictions = predict_binary(x_test_digits, theta_digits)
accuracy = compute_accuracy(predictions, y_test_digits)

print("accuracy: " + str(accuracy))
assert accuracy > 0.95





def plot_theta_image(theta, title=r"$\theta$ vector"):
    # remove bias from the image
    theta_no_bias = theta[:64].reshape(8,8)
    plt.figure()
    plt.gray()
    plt.title(title)
    plt.imshow(theta_no_bias)

plot_theta_image(theta_digits)
theta_digits








# Set learning rate (try experimenting with this)
alpha = 0.001

# Set theta to intial value of None
theta_digits = None

# We go through the entire training set a number of times
# Each of these iterations is called an epoch
n_epochs = 50

accuracies_train = []
accuracies_test = []
log_likelihoods_train = []
log_likelihoods_test = []

for epoch in range(n_epochs):
    theta_digits = train_theta(x_train_digits, y_train_digits, n_epochs=1, theta=theta_digits, alpha = alpha)
    # Calculate accuracy
    accuracy_train = -1
    accuracy_test = -1
    # START ANSWER
    predictions_train = predict_binary(x_train_digits, theta_digits)
    accuracy_train = compute_accuracy(predictions_train, y_train_digits)
    predictions_test = predict_binary(x_test_digits, theta_digits)
    accuracy_test = compute_accuracy(predictions_test, y_test_digits)
    # END ANSWER
    accuracies_train.append(accuracy_train)
    accuracies_test.append(accuracy_test)
    
    # Calculate log likelihood
    ll_train = 0
    ll_test = 0
    # START ANSWER
    h_x_train = hypothesis(x_train_digits, theta_digits)
    h_x_test = hypothesis(x_test_digits, theta_digits)
    
    ll_train = log_likelihood(h_x_train, y_train_digits)
    ll_test = log_likelihood(h_x_test, y_test_digits)
    # END ANSWER
    log_likelihoods_train.append(ll_train)
    log_likelihoods_test.append(ll_test)
        
plt.plot(np.arange(len(accuracies_train)), accuracies_train, label='train')
plt.plot(np.arange(len(accuracies_test)), accuracies_test, label='test')
plt.title('accurracy')
plt.xlabel('epoch')
plt.ylabel('accurracy')
plt.legend(loc=3)
plt.show()

plt.plot(np.arange(len(log_likelihoods_train)), log_likelihoods_train, label='train')
plt.plot(np.arange(len(log_likelihoods_test)), log_likelihoods_test, label='test')
plt.title('log likelihood')
plt.xlabel('epoch')
plt.ylabel('log likelihood')
plt.legend(loc=3)
plt.show()








# Import the load function for the dataset
from sklearn import datasets
from sklearn.model_selection import train_test_split

n_classes = 10

# Load the digits with 10 classes (digits 0 - 9)
all_digits = datasets.load_digits(n_class=n_classes)
all_digits_images = all_digits.images
all_digits_labels = all_digits.target

# Flatten the data so they are 1D and append extra ones to the feature vectors
all_digits_pixels = add_one_features(all_digits_images.reshape(all_digits_images.shape[0], -1))

# The shape should be (1797, 65)
assert all_digits_pixels.shape[0] == 1797
assert all_digits_pixels.shape[1] == 65

# Split dataset into train and test set
x_train_digits, x_test_digits, y_train_digits, y_test_digits = train_test_split(all_digits_pixels, all_digits_labels, test_size=0.3)





# Initialize a theta array, one for every class
multiclass_thetas = np.zeros((10,65))

for class_no in range(n_classes):
    current_label = class_no
    # Hint: convert the labels array to have only 1's at the current class_no
    # START ANSWER
    labels = (y_train_digits == current_label).astype(float)   # 0 or 1
    theta_digits = train_theta(x_train_digits, labels, n_epochs=10000, theta=theta_digits, alpha = 0.0001)
    multiclass_thetas[class_no] = theta_digits
    # END ANSWER
    print("class_no: " + str(class_no))

print("first 3 parameters of every theta")
print(multiclass_thetas[:,:3])





def predict_multiclass(x_test, theta):
    """
    Predicts a label for each image in x_test using theta.
    :param x_test: an array of size (n, 65) of all test images.
    :param theta: an (10,65) array of trained thetas.
    :return: an array of size (n,) of labels for each test_image.
    """
    predictions = np.zeros(x_test.shape[0], dtype=int)
    for i, x in enumerate(x_test):
        # START ANSWER
        probs = np.asarray([hypothesis(x, th) for th in theta])
        predictions[i] = np.argmax(probs)
        # END ANSWER
    return predictions

predictions = predict_multiclass(x_test_digits, multiclass_thetas)
# And print the accuracy
accuracy = compute_accuracy(predictions, y_test_digits)
print("accuracy: " + str(accuracy))

assert accuracy > 0.9





for i in range(n_classes):
    plot_theta_image(multiclass_thetas[i], str(i))





from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
import time

# We filter out the warnings which occur due to plotting the data at different contour levels
from warnings import filterwarnings
filterwarnings("ignore", "No contour levels*")


h = .02  # Step size in the mesh

names = ["Nearest Neighbors", "Logistic Regression"]

classifiers = [
    KNeighborsClassifier(3),
    LogisticRegression(multi_class='multinomial', solver='lbfgs')]

x7, y7 = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
x7 += 2 * rng.uniform(size=x7.shape)
linearly_separable = (x7, y7)

ds = make_moons(100, noise=0.3, random_state=0,)
figure = plt.figure(figsize=(20, 5))
i = 1
n_iterations = 10
for iteration in range(n_iterations):
    # Preprocess dataset, split into training and test part
    x7, y7 = ds
    x7 = StandardScaler().fit_transform(x7)
    x7_train, x7_test, y7_train, y7_test = train_test_split(x7, y7, test_size=.9, random_state=int(time.perf_counter()) + iteration)

    x7_min, x7_max = x7[:, 0].min() - .5, x7[:, 0].max() + .5
    y7_min, y7_max = x7[:, 1].min() - .5, x7[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x7_min, x7_max, h), np.arange(y7_min, y7_max, h))

    # Just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])

    # Iterate over classifiers
    c = 0
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(classifiers), n_iterations, i+c*n_iterations)
        clf.fit(x7_train, y7_train)
        score = clf.score(x7_test, y7_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]-0.5

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contour(xx, yy, Z, alpha=.8, levels=[0.5])

        # Plot the training points
        ax.scatter(x7_train[:, 0], x7_train[:, 1], c=y7_train, cmap=cm_bright,
                   edgecolors='k')

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        c+=1
    i += 1

plt.tight_layout()
plt.show()






