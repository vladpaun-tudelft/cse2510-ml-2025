














import numpy as np
from sklearn import datasets

iris = datasets.load_iris()
iris





print("First five flowers: \n", iris.data[:5, :])
print("Their labels: ", iris.target[:5])
print("And the label names: ", iris.target_names)

last_five_flowers = None
third_feature_only = None
first_ten_names = None
# START ANSWER
last_five_flowers = iris.data[-5:, :]
third_feature_only = iris.data[:, 2]
first_ten_names = iris.target_names[iris.target[:10]]
# END ANSWER

setosa_flowers = None
versicolor_flowers = None
virginica_flowers = None
# START ANSWER
setosa_flowers = iris.data[iris.target==0 , :]
versicolor_flowers = iris.data[iris.target==1 , :]
virginica_flowers = iris.data[iris.target==2 , :]
# END ANSWER


print("Last five flowers: \n", last_five_flowers)
print("Only the third feature: ", third_feature_only)
print("All label names: ", first_ten_names)
 
print("Class: ", iris.target_names[0], "; Items: \n", setosa_flowers)

assert last_five_flowers.shape == (5,4), "Expected a two dimensional array of shape (5,4)"
assert third_feature_only.shape == (150,), "Expected an array of shape (150,)"
assert first_ten_names.shape == (10,), "Expected an array of shape (10,)"

assert setosa_flowers.shape == (50,4), "Expected a two dimensional array of shape (50,4)"
assert versicolor_flowers.shape == (50,4), "Expected a two dimensional array of shape (50,4)"
assert virginica_flowers.shape == (50,4), "Expected a two dimensional array of shape (50,4)"





# From the Matplotlib library, import pyplot. We will refer to this library later as plt.
# This is a widely used library that lets you create images and plot your data.
from matplotlib import pyplot as plt

# Create a scatterplot of the first two features, and use their labels as colour values.
plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target, alpha=0.7)
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.show()
# Create a scatterplot of the third and fourth feature.
plt.scatter(iris.data[:, 2], iris.data[:, 3], c=iris.target, alpha=0.7)
plt.xlabel(iris.feature_names[2])
plt.ylabel(iris.feature_names[3])
plt.show()








from sklearn.model_selection import train_test_split #to split in train and test set

# load the data and create the training and test sets
iris = datasets.load_iris()
# X is the feature vectors for the data points, and Y is the target (ground truth) class for those data points 
# the iris.data and iris.target entries are randomly divided into training and test sets.
X_train, X_test, Y_train, Y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=20)

# Due to the randomness of the split, number of each flowers is not necessarily the same
# Separate the training dataset into the three flower types.
setosa_X_train = None
versicolor_X_train = None
virginica_X_train = None
# START ANSWER
setosa_X_train = X_train[Y_train==0, :]
versicolor_X_train = X_train[Y_train==1, :]
virginica_X_train = X_train[Y_train==2, :]
# END ANSWER

assert setosa_X_train.shape[0] != versicolor_X_train.shape[0]
assert setosa_X_train.shape[0] != virginica_X_train.shape[0]
assert versicolor_X_train.shape[0] != virginica_X_train.shape[0]

setosa_X_train.shape, versicolor_X_train.shape, virginica_X_train.shape





# We use the third feature
feature_idx = 2





plt.hist(setosa_flowers[:,feature_idx], label=iris.target_names[0], alpha=0.7)
plt.hist(versicolor_flowers[:,feature_idx], label=iris.target_names[1], alpha=0.7)
plt.hist(virginica_flowers[:,feature_idx], label=iris.target_names[2], alpha=0.7)
plt.xlabel(iris.feature_names[feature_idx])
plt.ylabel('Number of flowers')
plt.legend()
plt.show()





def compute_mean(x):
    mean = 0
    # START ANSWER
    mean = np.sum(x) / x.shape[0]
    # END ANSWER
    return mean
    
def compute_sd(x, mean):
    sd = 0
    # START ANSWER
    sd = np.sqrt(np.sum(np.power(x - mean, 2)) / x.shape[0])
    # END ANSWER
    return sd

# Compute the mean for each flower type.
mean_setosa = compute_mean(setosa_X_train[:, feature_idx])
mean_versicolor = compute_mean(versicolor_X_train[:, feature_idx])
mean_virginica = compute_mean(virginica_X_train[:, feature_idx])

# Compute the standard deviation for each flower type.
sd_setosa = compute_sd(setosa_X_train[:, feature_idx], mean_setosa)
sd_versicolor = compute_sd(versicolor_X_train[:, feature_idx], mean_versicolor)
sd_virginica = compute_sd(virginica_X_train[:, feature_idx], mean_virginica)

# Print the computed means and standard deviations.
print("setosa", mean_setosa, sd_setosa)
print("versicolor", mean_versicolor, sd_versicolor)
print("virginica", mean_virginica, sd_virginica)

assert np.isclose(mean_setosa, 1.4729729729729728), "Expected a different mean"
assert np.isclose(mean_versicolor, 4.25), "Expected a different mean"
assert np.isclose(mean_virginica, 5.572222222222222), "Expected a different mean"

assert np.isclose(sd_setosa, 0.17652600857089654), "Expected a different standard deviation"
assert np.isclose(sd_versicolor, 0.44300112866673375), "Expected a different standard deviation"
assert np.isclose(sd_virginica, 0.547017728288333), "Expected a different standard deviation"








from scipy.stats import norm

def normal_PDF(x, mean, sd):
    pdf = 0
    # START ANSWER
    pdf = (1 / np.sqrt(2 * np.pi * (sd ** 2))) * np.exp(-(x - mean) ** 2 / (2 * (sd ** 2)))
    # END ANSWER
    return pdf

# Set x, mean and standard deviation
x = 0.5
mean = 2
sd = 0.5
my_pdf = normal_PDF(x, mean, sd)

# You can compare your outcome to scipy's built-in normal PDF
scipy_pdf = norm.pdf(x, mean, sd)
print("Your pdf function outcome: ", my_pdf, " Scipy's function outcome: ", scipy_pdf)
assert np.isclose(my_pdf, scipy_pdf)

# And we plot the result of your PDF function for 100 points between 0 and 4: np.linspace(0, 4, 100)
xs = np.linspace(0, 4, 100)
plt.plot(xs, normal_PDF(xs, mean, sd))
plt.show()





# Histograms of the flower types of the training set
plt.hist(setosa_X_train[:,feature_idx], label=iris.target_names[0], alpha=0.7)
plt.hist(versicolor_X_train[:,feature_idx], label=iris.target_names[1], alpha=0.7)
plt.hist(virginica_X_train[:,feature_idx], label=iris.target_names[2], alpha=0.7)

# Plot your PDFs here
xs = np.linspace(0, 7, 100)
# START ANSWER
plt.plot(xs, normal_PDF(xs, mean_setosa, sd_setosa), label=iris.target_names[0] + ' pdf', alpha=0.7)
plt.plot(xs, normal_PDF(xs, mean_versicolor, sd_versicolor), label=iris.target_names[1] + ' pdf', alpha=0.7)
plt.plot(xs, normal_PDF(xs, mean_virginica, sd_virginica), label=iris.target_names[0] + ' pdf', alpha=0.7)
# END ANSWER

plt.xlabel(iris.feature_names[feature_idx])
plt.ylabel('Number of flowers / PDF')
plt.legend()
plt.show()








def posterior(x, means, sds, priors, i):
    """
    Compute the posterior probability P(C_i | x).
    :param x: the sample to compute the posterior probability for.
    :param means: an array of means for each class.
    :param sds: an array of standard deviation values for each class.
    :param priors: an array of frequencies for each class.
    :param i: the index of the class to compute the posterior probability for.
    """
    posterior = 0
    # START ANSWER
    p_x = 0
    for j in range(len(priors)):
        p_x += normal_PDF(x, means[j], sds[j]) * priors[j]
    posterior = normal_PDF(x, means[i], sds[i]) * priors[i] / p_x
    #END ANSWER
    return posterior

means = [mean_setosa, mean_versicolor, mean_virginica]
sds = [sd_setosa, sd_versicolor, sd_virginica]
priors = [
    setosa_X_train.shape[0]/X_train.shape[0],
    versicolor_X_train.shape[0]/X_train.shape[0],
    virginica_X_train.shape[0]/X_train.shape[0]
]

# Test out the code
flower_idx = 6
print("Flower belongs to class", iris.target_names[Y_train[flower_idx]])

# iterate over all classes
for i in range(3):
    x_post = posterior(X_train[flower_idx, feature_idx], means, sds, priors, i)
    print("Posterior probability for class", iris.target_names[i], ": ", x_post)

post_setosa = posterior(X_train[flower_idx, feature_idx], means, sds, priors, 0)
post_versicolor = posterior(X_train[flower_idx, feature_idx], means, sds, priors, 1)
post_virginica = posterior(X_train[flower_idx, feature_idx], means, sds, priors, 2)

assert np.isclose(post_setosa, 1.1048294835009998e-107, rtol = 0.0001, atol = 0.), "Expected a different posterior probability"
assert np.isclose(post_versicolor, 0.03817178391547811, rtol = 0.0001, atol = 0.), "Expected a different posterior probability"
assert np.isclose(post_virginica, 0.9618282160845218, rtol = 0.0001, atol = 0.), "Expected a different posterior probability"





xs = np.linspace(0, 7, 100)
# START ANSWER
plt.plot(xs, posterior(xs, means, sds, priors, 0), label=iris.target_names[0])
plt.plot(xs, posterior(xs, means, sds, priors, 1), label=iris.target_names[1])
plt.plot(xs, posterior(xs, means, sds, priors, 2), label=iris.target_names[2])
# END ANSWER
plt.xlabel(iris.feature_names[feature_idx])
plt.ylabel('Posterior probability')
plt.legend()
plt.show()








def classify(x, means, sds, priors):
    classification = -1
    # START ANSWER
    probs = []
    for i in range(len(priors)):
        probs.append(posterior(x, means, sds, priors, i))
    classification = probs.index(max(probs))
    # END ANSWER
    return classification

# Test out the code
flower_idxs = [5,20,30]
predicted_classes = np.zeros(3, dtype=np.int64)
for i, flower_idx in enumerate(flower_idxs):
    predicted_classes[i] = classify(X_train[flower_idx, feature_idx], means, sds, priors)

print("Predicted class", iris.target_names[predicted_classes])
print("Flower belongs to class", iris.target_names[Y_train[flower_idxs]])
assert (predicted_classes == Y_train[flower_idxs]).all()





def evaluate(X_test, Y_test, means, sds, priors):
    accuracy = 0
    # START ANSWER
    for i in range(X_test.shape[0]):
        accuracy += 1 if classify(X_test[i], means, sds, priors) == Y_test[i] else 0
    accuracy /= X_test.shape[0]
    # END ANSWER
    return accuracy

accuracy = evaluate(X_test[:, feature_idx], Y_test, means, sds, priors)

print(accuracy)
assert accuracy > 0.9, "Expected a higher accuracy"





def decision_boundary(means, sds, priors):
    decision_boundaries = []
    # START ANSWER
    xs = np.linspace(1,7,1000)
    for x in xs:
        p0=posterior(x,means,sds,priors,0)
        p1=posterior(x,means,sds,priors,1)
        p2=posterior(x,means,sds,priors,2)
        if any_equal(p0,p1,p2): decision_boundaries.append(x)
    # END ANSWER
    return decision_boundaries

# Create a scatterplot of the third and fourth feature.
def any_equal(a, b, c, atol=0.05):
    return ((np.isclose(a,b,atol=atol) and a > 0.01 and b > 0.01) or
            (np.isclose(a,c,atol=atol) and a > 0.01 and c > 0.01) or
            (np.isclose(b,c,atol=atol) and b > 0.01 and c > 0.01))
feature_idx2 = 3

plt.scatter(iris.data[:, feature_idx], iris.data[:, feature_idx2], c=iris.target, alpha=0.7)
plt.xlabel(iris.feature_names[feature_idx])
plt.ylabel(iris.feature_names[feature_idx2])
decision_boundaries = decision_boundary(means, sds, priors)
for boundary in decision_boundaries:
    plt.axvline(x=boundary)

plt.show()








import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

# Load data
iris = datasets.load_iris()
X = iris.data[:, [0, 2]]  # two features for easy plotting
y = iris.target
classes = np.unique(y)

# Estimate Gaussian parameters
means = [X[y == c].mean(axis=0) for c in classes]
covs = [np.cov(X[y == c].T) for c in classes]
priors = [np.mean(y == c) for c in classes]

def gaussian_density(x, mean, cov):
    d = len(x)
    det = np.linalg.det(cov)
    inv = np.linalg.inv(cov)
    norm_const = 1.0 / np.sqrt((2*np.pi)**d * det)
    return norm_const * np.exp(-0.5 * (x - mean).T @ inv @ (x - mean))

def predict(x):
    posteriors = [gaussian_density(x, means[c], covs[c]) * priors[c] for c in classes]
    return np.argmax(posteriors)

# Plot decision regions
xx, yy = np.meshgrid(np.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),
                     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))
Z = np.array([predict(np.array([a, b])) for a, b in zip(xx.ravel(), yy.ravel())])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(X[:,0], X[:,1], c=y, edgecolor="k")
plt.xlabel("Sepal length")
plt.ylabel("Petal length")
plt.title("Gaussian density-based classifier")
plt.show()



import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

idx1 = 2
idx2 = 3

iris = datasets.load_iris()
Y = iris.target
classes = np.unique(Y)

def get_posteriors(dataset, idx):
    X = dataset.data[:, idx]
    Y = dataset.target
    classes = np.unique(Y)
    
    means = [X[Y==c].mean() for c in classes]
    sds = [np.std(X[Y==c]) for c in classes]
    priors = [np.mean(Y == c) for c in classes]
    priors = np.asarray(priors).reshape(3,1)
    
    xs = np.linspace(0, X[:].max()+1, 200)
    
    densities = np.asarray([(1 / np.sqrt(2 * np.pi * (sds[c] ** 2))) * np.exp(-(xs - means[c]) ** 2 / (2 * (sds[c] ** 2))) for c in classes])
    
    posteriors = densities * priors / (densities * priors).sum(axis=0, keepdims=True)
    predicitons = np.argmax(posteriors, axis=0)
    return (xs, posteriors, predicitons)

(xs, posteriorsX, predX) = get_posteriors(iris, idx1)
(ys, posteriorsY, predY) = get_posteriors(iris, idx2)

cmap = plt.get_cmap("viridis", len(classes))

plt.scatter(iris.data[:,idx1], iris.data[:,idx2], c=iris.target, alpha=0.7)

plt.xlabel(iris.feature_names[idx1])
plt.ylabel(iris.feature_names[idx2])

for c in range(len(classes)):
    plt.plot(xs, posteriorsX[c], color=cmap(c), label=iris.target_names[c])
    plt.plot(posteriorsY[c], ys, color=cmap(c), label=iris.target_names[c])

change_x = np.where(np.diff(predX) != 0)[0]
change_y = np.where(np.diff(predY) != 0)[0]

for i in change_x:
    plt.axvline(xs[i], ls='--', alpha=0.5)
for i in change_y:
    plt.axhline(ys[i], ls='--', alpha=0.5)

from matplotlib.colors import ListedColormap
colors = cmap(np.arange(len(classes)))
extent = (xs.min(), xs.max(), ys.min(), ys.max())

for c in range(len(classes)):
    mask = (predX[None, :] == c) & (predY[:, None] == c)   # shape (len(ys), len(xs))
    # colormap with transparent "off" and class color "on"
    cm = ListedColormap([(0,0,0,0), (*colors[c][:3], 0.15)])  # 15% opacity
    plt.imshow(mask.astype(int), origin="lower", extent=extent, aspect="auto", cmap=cm, interpolation="nearest")

plt.legend()
plt.show()







