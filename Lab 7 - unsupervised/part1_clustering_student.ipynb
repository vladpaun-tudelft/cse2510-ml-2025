{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Unsupervised Learning (Part 1: Clustering)\n",
    "CSE2510 Machine Learning 2025/2026 <br>\n",
    "*CSE Machine Learning Teaching Team*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming exercises and insight questions on k-means clustering. \n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of unsupervised learning.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. There is quite a bit of theory and explanation in these notebooks. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. For questions and feedback please consult the TAs during the lab session. \n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning without examples\n",
    "\n",
    "When we want to learn from data without knowing the labels, we apply unsupervised learning: different techniques to make sense of data from the data itself. An example of a task where unsupervised learning can be successfully applied is dimensionality reduction: trying to find the 'essential' features or combinations of features to describe objects. In Assignment 7 you will practice with k-means clustering and implement PCA to reduce dimensionality of data.\n",
    "\n",
    "\n",
    "### Structure\n",
    "\n",
    "The assignment consists of two parts. Each of these parts is presented in a different Jupyter notebook, you can move between the notebooks by clicking on the Jupyter logo in the top left corner.\n",
    "\n",
    "- In Part 1 you will learn how to perform k-means clustering. It consists of two steps:\n",
    "\n",
    "   1. Preparing the data: extract the features from available data.\n",
    "   2. K-means clustering: implement k-means and apply it on the retrieved features.  \n",
    "</br>\n",
    "\n",
    "- In Part 2 you will become familiar with dimensionality reduction using Principal Component Analysis (PCA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "As a child you probably taught yourself how to separate building blocks from each other. Even without someone telling you which shape was which, you were able to decide which shapes belonged with each other and make separate heaps of cubes, cylinders, and pyramids. You performed a type of unsupervised learning: *clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the data\n",
    "\n",
    "In this assignment, you will work with the *TIDIGITS* dataset. This dataset was created by Texas Instruments (hence, TI) and is a set of voice recordings of digits. You can go ahead and give the audio files a listen in the data folder: `data/tidigits/...`.\n",
    "\n",
    "Before we can use any clustering algorithms on this data, we need a way to describe our audio files in a way that a computer can understand. An audio file is read as an array, where each value in the array is the amplitude of the audio at a the corresponding time. We refer to this data as _raw_ waveforms. It's infeasible to use the raw data for clustering. We need to extract a limited number of features to describe each audiofile: we perform __feature extraction__.\n",
    "\n",
    "From the audiofiles we extracted _MFCC_ features, which are often used for speech processing. We split the audio file in frames of 25 ms each and applied a number of complicated operations to retrieve 13 features per frame. If a file is split into, for example, 100 frames, this means we have $13 * 100 = 1300$ features! To bring this number down, we further sampled  5  frames from regular intervals (the size of the interval is dependent on the length of the audio file) and flattened this to an array of  $5 âˆ— 13 = 65$  features (this number is thus independent of the length of the audio file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.1}$ Run the code below to load the extracted MFCC features for the 150 files provided. This will give you a dataset with 50 'one' audiofiles, 50 'two' audiofiles and 50 'three' audiofiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.loadtxt(\"data/tidigits_features.txt\")\n",
    "y_train = np.loadtxt(\"data/tidigits_targets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the number of features\n",
    "\n",
    "Now we have 150 data points (i.e. 150 audio files) and each data point consists of 65 features describing the audio file. Let's plot the the first two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.title('The first two features of the data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems like we can't cluster our points based on the first two features. There is however some combination of features that will allow us to cluster the data points. If only there was a way to automatically find this combination without supervision...\n",
    "\n",
    "But wait! We do have such a technique: PCA. You will implement your own version of this algorithm in the second part of the assignment; for now we will just make use of an available implementation. We will find the four most descriptive 'directions' in the data and project the features onto these directions.\n",
    "\n",
    "This involves the following steps:\n",
    "1. compute the covariance matrix of the dataset;\n",
    "2. compute the first four eigenvectors (i.e. principal components) of the covariance matrix;\n",
    "3. reorient the data points from the original axes to the ones represented by these principal components (this is done using the dot product).\n",
    "   \n",
    "This should give us four features for each point that describe the audio well. We are plotting just two of these features, as this can be easily visualised, but we will need four features to accurately cluster all points.\n",
    "\n",
    "$\\ex{1.2}$ Complete the following code to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First compute the covariance matrix of the dataset\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Next, retrieve four eigenvectors of the covariance matrix using np.linalg.eig()\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "\n",
    "# Finally, we project our points onto the eigenvectors (principal components) using matrix multiplication\n",
    "X_train_reduced = np.zeros((X_train.shape[0], 4))\n",
    "for i in range(len(X_train)):\n",
    "    X_train_reduced[i, :] = np.matmul(X_train[i, :].T, eigenvectors).T\n",
    "        \n",
    "# And plot the points with the new data\n",
    "plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], c=y_train)\n",
    "plt.title('The data represented by the principal components')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have ourselves a dataset that we can start clustering!\n",
    "\n",
    "__Important note:__ We have now shown the labels for each point as the colour, but we will not be using labels for training. You will only use the labels to verify the clusters you are deriving from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-means clustering\n",
    "\n",
    "We will now implement the (original) k-means clustering algorithm. This algorithm works as follows: we start with $k$ clusters and pick some random centre (mean) for each cluster. Next, we assign points to each cluster based on the distance to the centres and we update the centre to be the mean of all points in that cluster. This is repeated until the centres have converged.\n",
    "\n",
    "Pointwise, the steps of the k-means clustering algorithm approach are as follows:\n",
    "1. initialize $k$ cluster centers at random locations;\n",
    "2. assign each point to a cluster;\n",
    "3. update the cluster centers;\n",
    "4. go to step 2 unless converged or a certain number of iterations has been reached.\n",
    "\n",
    "First, we will create a `Cluster` class.\n",
    "\n",
    "$\\ex{1.1}$ Finish the method that calculates the centroid (other word for centre) of the cluster,\n",
    "`Cluster.centroid`. The centroid is the mean vector of all feature vectors. This method is called frequently, while the cluster (and naturally the centroid) does not always change. To increase efficiency, store and reuse the cluster centroid until the cluster itself changes (i.e. points are added or removed), at which point a new centroid should be calculated.\n",
    "\n",
    "__Hint:__ You can use the ``changed`` flag to see if the centroid needs to be recomputed or if the current value is still valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This object is used to store the clusters. A Cluster object consists of a numpy matrix\n",
    "# containing all feature vectors in the cluster and the centroid of all the vectors.\n",
    "# The object also contains a boolean for speedup purposes.\n",
    "class Cluster(object):\n",
    "\n",
    "    def __init__(self, array=np.array([])):\n",
    "        self.changed = True\n",
    "        self.data = np.array(array)\n",
    "        self.cd = self.data\n",
    "        \n",
    "    def reset_cluster(self):\n",
    "        self.data = np.array([])\n",
    "    \n",
    "    def is_changed(self):\n",
    "        return self.changed\n",
    "    \n",
    "    def set_changed(self, changed):\n",
    "        self.changed = changed\n",
    "    \n",
    "    def set_centroid(self, vector):\n",
    "        self.cd = vector\n",
    "    \n",
    "    def append(self, other):\n",
    "        # Set changed flag to true (the cluster has changed)\n",
    "        self.set_changed(True)\n",
    "        \n",
    "        self.data = np.vstack((self.data, other))\n",
    "\n",
    "    def centroid(self):\n",
    "        # If the matrix consists of 1 vector, no need to compute centroid.\n",
    "        if len(np.shape(self.data)) == 1:\n",
    "            return self.data\n",
    "        # Check whether the cluster has changed since last computation (for speedup)\n",
    "        # and update the changed flag.\n",
    "        # START ANSWER\n",
    "        # END ANSWER\n",
    "        return self.cd\n",
    "    \n",
    "# Test case for the Cluster class\n",
    "c = Cluster(np.array([[0, 1], [2, 0]]))\n",
    "\n",
    "# Verifies that the centroid is calculated correctly\n",
    "np.testing.assert_array_equal(c.centroid(), np.array([1.0, 0.5]))\n",
    "\n",
    "# Verifies that the centroid is calculated correctly after a new data point has been added.\n",
    "c.append(np.array([1, 2]))\n",
    "np.testing.assert_array_equal(c.centroid(), np.array([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the data that we will be clustering. We plotted the data here as the algorithm will see it, thus without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# This function is only used when the previous steps for feature extraction and reduction have failed.\n",
    "# It can be used to load the given cluster data into a list of cluster objects.\n",
    "def read_file(file):\n",
    "    lines = [line.rstrip('\\n') for line in open(file)]\n",
    "    # Initialise data structure\n",
    "    points = []\n",
    "    for line in lines:\n",
    "        # Take FeatureVector from dataset\n",
    "        elements = line.split(\" \")\n",
    "        p = [float(el) for el in elements]\n",
    "        # Append FeatureVector to the list of clusters\n",
    "        points.append(p)\n",
    "    points = np.asarray(points)\n",
    "    return points\n",
    "\n",
    "# If the previous steps for feature extraction and reduction failed, you can use this data.\n",
    "# Otherwise, use the data and features you extracted yourself!\n",
    "\n",
    "# points = read_file(\"data/cluster.txt\")\n",
    "\n",
    "points = X_train_reduced\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "plt.scatter(x, y, c='r')\n",
    "plt.title('The data to be clustered')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Now you will start implementing k-means. Let's work on step 1: *initialize $k$ cluster centers at random locations*.\n",
    "\n",
    "$\\ex{1.2}$  Finish the `addInitPoints` function. This function selects a random point in the dataset, constructs a cluster from it and adds the cluster to the list of clusters.  \n",
    "**Hint:** We imported the `sample()` method, you can use it to create a list of elements drawn at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "# This function selects random k points from the dataset. For each random point it initializes a cluster \n",
    "# and adds the cluster to the list of clusters.\n",
    "def add_init_points(points, clusters, k):\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return clusters\n",
    "\n",
    "# Verifies that a cluster has been added as a init point\n",
    "init_points = add_init_points(np.array([[0, 0], [0, 0]]), [], 1)\n",
    "assert len(init_points) == 1\n",
    "np.testing.assert_array_equal(init_points[0].centroid(), np.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "In the update process, we assign points to clusters based on their distance to each centroid. This is step 2 of k-means: *assign each point to a cluster*.\n",
    "\n",
    "$\\ex{1.3}$ Implement the `distance` function. We will use Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    # Euclidian distance between 2 points (in any space)\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "\n",
    "# Verifies that the distance metric is correct\n",
    "np.testing.assert_array_equal(distance(np.zeros([1, 2]), np.ones([1, 2])), np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "$\\ex{1.4}$ Now implement the update step (3.) of the k-means clustering algorithm. First, use `addInitPoint` if no clusters are present yet to initialize $k$ clusters. After this check, we perform an iteration of the k-means clustering algorithm. This consists of the following steps: \n",
    "1. calculate the centroids of each cluster and save them;\n",
    "2. remove all points from all clusters;\n",
    "3. add each point to the closest cluster centroid (the centroids that you saved earlier).\n",
    "\n",
    "You will want to save the cluster centroids as they will change during the last step when they are recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# This function updates the list of k clusters\n",
    "def update_k_means(points, clusters, k):\n",
    "    # Reset clusters   \n",
    "    centroids = []\n",
    " \n",
    "    # Add initial points\n",
    "    # If add_clusters is true, initialise clusters with add_init_points\n",
    "    # Then add the cluster centroids to the centroids list\n",
    "    add_clusters = len(clusters) == 0\n",
    "    # START ANSWER    \n",
    "    # END ANSWER\n",
    "\n",
    "    # Reset clusters from last iteration,\n",
    "    # so the clustering can be performed with new centroids\n",
    "    for cluster in clusters:\n",
    "        cluster.reset_cluster()\n",
    "        \n",
    "    clusters = [None for el in range(k)]\n",
    "    for p in points:\n",
    "        # Calculate the min distance to one of the centroids\n",
    "        # START ANSWER  \n",
    "        # END ANSWER  \n",
    "    \n",
    "        # Add the data point to the cluster with min_distance to centroid\n",
    "        if label >= 0:\n",
    "            if clusters[label] is None:\n",
    "                clusters[label] = Cluster(p)\n",
    "            else:\n",
    "                clusters[label].append(p)\n",
    "         \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "$\\ex{1.5a}$ Use the `plot_k_means_data` function to plot the clustered data and have it analyse the *TIDIGITS* dataset (or `data/cluster.txt` if this is not possible) with $k$ set to 3. Run the k-means clustering algorithm and verify that it is clustering the points as intended. Use `plot_k_means_data_update` to see the cluster updates at each iteration. Note that you may need to run your algorithm with a different random seed to get a proper clustering.\n",
    "\n",
    "__Hint:__ Updating the clusters is a stateful operation, so if you want to keep the previous cluster you should make a deep copy of it using `copy.deepcopy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "    \n",
    "def plot_k_means_data( clusters, k, itr):\n",
    "    colors = cm.brg(np.linspace(0,1,k))\n",
    "    for (i,cl) in enumerate(colors):\n",
    "        x = [[el[0]] for el in clusters[i].data]\n",
    "        y = [[el[1]] for el in clusters[i].data] \n",
    "        plt.scatter(x, y, c=[cl])\n",
    "        plt.scatter(clusters[i].centroid()[0], clusters[i].centroid()[1], c='black')\n",
    "        plt.title(\"Clusters at update \" + str(itr))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_k_means_data_update(clusters_prev, clusters, k):\n",
    "    if(clusters_prev == []):\n",
    "        return\n",
    "    colors = cm.brg(np.linspace(0,1,k))\n",
    "    for (i,cl) in enumerate(colors):\n",
    "        \n",
    "        x = [[el[0]] for el in clusters[i].data]\n",
    "        y = [[el[1]] for el in clusters[i].data] \n",
    "        plt.scatter(x, y, c=[cl], marker = '*')\n",
    "\n",
    "        plt.scatter(clusters[i].centroid()[0], clusters[i].centroid()[1], c='black')\n",
    "        \n",
    "        x_prev = [[el[0]] for el in clusters_prev[i].data]\n",
    "        y_prev = [[el[1]] for el in clusters_prev[i].data]\n",
    "        \n",
    "        plt.scatter(x_prev, y_prev, c=[cl], alpha=0.2, s=100)\n",
    "        \n",
    "        plt.scatter(clusters_prev[i].centroid()[0], clusters_prev[i].centroid()[1], c='green')\n",
    "        plt.legend(['New clusters', 'New centroids', 'Previous clusters', 'Previous centroids'], \n",
    "                   loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.title('Difference between updates')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "points = X_train_reduced # Ex 1.5a\n",
    "# points = X_train # Ex 1.5b (optional)\n",
    "clusters = []\n",
    "clusters_prev = []\n",
    "centroids = []\n",
    "k = 3\n",
    "centroids_prev = [np.zeros((points.shape[1])) for x in range(0,k)]\n",
    "\n",
    "# Run here the update_k_means function with k=3 using the clusters list. \n",
    "# Keep updating until there is no change in centroids in consecutive iterations \n",
    "# or a maximum number of iterations (e.g. 10) is reached.\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER\n",
    "        \n",
    "# Ground truth\n",
    "plt.scatter(points[:, 0], points[:, 1], c=y_train)\n",
    "plt.title('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.5b (Optional)}$ Try using the clustering on the raw audiofiles (uncomment the `points = X_train` line). How does this approach compare to the extracted features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ How do the clusters computed using k-means compare to the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Now that we have a cluster, we would like to find out how good our clustering is. We will also do this using an unsupervised approach: without knowing the correct labels for each cluster, we _can_ say something about the spread of each cluster.\n",
    "\n",
    "For this purpose you will compute the _sum of residual squares_ (SRS) of the cluster. This is computed as follows: sum over the squared euclidean distances between each point and their corresponding centroid and divide by the total number of points. In math notation:\n",
    "\n",
    "$$\n",
    "    srs = \\frac{1}{N} \\sum_{i \\in C} (p_i - c)^2\n",
    "$$\n",
    "\n",
    "with $N$ the number of points, $C$ the cluster containing point $i$ and centroid $c$, and $p_i$ the feature vector for point $i$.\n",
    "\n",
    "$\\ex{1.6}$ Implement the `calculate_average_sum_rs` function, which computes the average sum of residual squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the average sum of residual squares of the given cluster\n",
    "def calculate_average_sum_rs(cluster):\n",
    "    if len(cluster.data) == 0:\n",
    "        return None\n",
    "    # START ANSWER\n",
    "    # END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Next, we will use this metric to try and automatically determine how many clusters we should use.\n",
    "\n",
    "$\\ex{1.7}$ Implement the `tune_k` method. This method should test out several values for $k$ in range from 1 to 15. Then, for each $k$, run the k-means algorithm on `data/cluster.txt` with $10$ update iterations. After the algorithm is done, calculate the average SRS of all clusters and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# This function tries a number of k's for the update_k_means function (which is our kMean classifier)\n",
    "# and calculates the SRS for each k. \n",
    "# For each k, n_updates iterations of the update_k_means function are performed.\n",
    "def tune_k(min_k, max_k, n_updates):\n",
    "    assert 0 < min_k < max_k\n",
    "    assert n_updates > 1\n",
    "    srss = []\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    plt.plot(list(range(min_k, (max_k+1))), srss, marker='o')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum of Residual Squares')\n",
    "    plt.grid(linestyle='-', linewidth=1)\n",
    "    plt.show()\n",
    "    \n",
    "min_k=1\n",
    "max_k=15\n",
    "n_updates=10\n",
    "tune_k(min_k, max_k, n_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.2}$ What seems to be a good choice for $k$, judging by the results? A lower SRS means each cluster is more compact.  \n",
    "**Note:** The optimal value of $k$ can be selected using the *elbow method*.\n",
    "\n",
    "$\\q{1.3}$ What would be the average SRS if we set $k$ equal to the number of points in our dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
