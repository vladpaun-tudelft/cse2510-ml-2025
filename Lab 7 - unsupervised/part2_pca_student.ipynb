{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Unsupervised Learning (Part 2: PCA)\n",
    "CSE2510 Machine Learning 2025/2026 <br>\n",
    "*CSE Machine Learning Teaching Team*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming exercises and insight questions on Principal Component Analysis. \n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of unsupervised learning.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. There is quite a bit of theory and explanation in these notebooks. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. For questions and feedback please consult the TAs during the lab session. \n",
    "\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning without examples (continued)\n",
    "\n",
    "\n",
    "### Structure\n",
    "\n",
    "The assignment consists of two parts. Each of these parts is presented in a different Jupyter notebook, you can move between the notebooks by clicking on the Jupyter logo in the top left corner.\n",
    "\n",
    "- In Part 1, you have practiced with k-means clustering.\n",
    "<br>\n",
    "- In this part you will get familiar with dimensionality reduction using Principal Component Analysis (PCA). You will implement PCA to perform dimensionality reduction on datasets. Your algorithm will be used to reduce the dimensions of a 2D Gaussian dataset as well as a set of images of faces. This part consists of two steps:\n",
    "\n",
    "   1. Power Iteration: calculate eigenvectors.\n",
    "   2. Principal Component Analysis: find principal components using power iteration.\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Power Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are characteristic vectors of a data set. \n",
    "Consider a square $(D × D)$ matrix $M$, and a $D × 1$ column vector $\\mathbf{v}$ which is an eigenvector of $M$, with eigenvalue $λ ∈ \\mathbb{R}$.\n",
    "\n",
    "$\\q{1.1}$ What equation defines the relation between $M$, $\\mathbf{v}$, and $λ$?  Write down this equation.\n",
    "\n",
    "The Power Iteration method is a relatively simple method for calculating eigenvectors for a square $(D × D)$ matrix $M$. The process works as follows:  \n",
    "A. Construct a vector $\\mathbf{v}_0$ of ones of length $D × 1$.  \n",
    "B. Until convergence, compute:  \n",
    "\n",
    "$\\mathbf{v}_{k+1} = \\dfrac{M \\cdot\\mathbf{v}_{k}}{||M \\cdot \\mathbf{v}_{k}||}$, where  \n",
    "$||M \\cdot \\mathbf{v}_{k}||$ is the L2 norm of $M\\mathbf{v}_{k}$.\n",
    "\n",
    "C. Output vector $\\mathbf{v}$ as the principal eigenvector of $M$.  \n",
    "D. Compute $M^∗$ as:  \n",
    "\n",
    "$λ = \\mathbf{v}^\\intercal \\cdot M \\cdot \\mathbf{v}$  \n",
    "$M^∗ = M − λ × \\mathbf{v \\cdot v}^\\intercal$\n",
    "\n",
    "E. If more eigenvectors are required, go to step 1 with $M^∗$ as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.1}$ Finish the `power_iteration()` function. This method will calculate `n_vectors` eigenvectors from the square matrix `matrix`. Use the steps described above to do so. The stopping criterion in the second step should be when the L2 norm of the difference between $v_{k+1}$ and $v_k$ of two consecutive iterations is smaller than the convergence parameter `e`.<br>\n",
    "**Hint:** The L2 norm can be computed with `la.norm(x)` which is the same as computing $\\sqrt{x^2}$.<br>\n",
    "**Hint:** There are several ways to multiply vectors and matrices with numpy (such as `np.dot()`, `np.matmul()`, or `np.outer()`). Whichever way you work with, make sure that you take into account the shapes of your vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "def power_iteration(matrix, n_vectors, e):\n",
    "    \"\"\"\n",
    "    This function returns a list with `n_vectors` amount of eigenvectors (numpy vectors) based on the given square \n",
    "    `matrix` and the convergence parameter `e`.\n",
    "    :param matrix: the square matrix\n",
    "    :param n_vectors: the number of eigenvectors\n",
    "    :param e: the convergence parameter\n",
    "    :return: the list of eigenvectors found\n",
    "    \"\"\"\n",
    "    assert (matrix.shape[0] == matrix.shape[1] & matrix.shape[1] >= n_vectors)\n",
    "\n",
    "    eigen_vectors = list()\n",
    "    \n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    \n",
    "    return eigen_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.2}$ How many eigenvectors could there possibly be for a $D × D$ matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next exercise, you will need to read data from a file. Use the `read_data` function below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    \"\"\"\n",
    "    This function loads a given matrix data file into a numpy matrix.\n",
    "    :param file_name: name of the file to be read\n",
    "    :return: the data as a numpy array\n",
    "    \"\"\"\n",
    "    lines = [line.rstrip('\\n') for line in open(file_name)]\n",
    "\n",
    "    result = np.zeros((len(lines), len(lines[0].split(\" \"))))\n",
    "\n",
    "    for (i, line) in enumerate(lines):\n",
    "        line = line.split(\" \")\n",
    "        for (j, number) in enumerate(line):\n",
    "            result[i][j] = float(number)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\ex{1.2}$ Create a matrix and let it read the data from `data/matrix.txt`. Use the `power_iteration()` method to calculate two eigenvectors from this matrix (and set `e` to a reasonably small value, e.g. $10E-5$). Verify that the resulting eigenvectors are roughly equal to:\n",
    " \n",
    "$v_1 = \\begin{bmatrix}0.4472 \\\\ 0.8944\\end{bmatrix}$\n",
    "$v_2 = \\begin{bmatrix}0.8944 \\\\ -0.4472\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and call the power_iteration function for this exercise.\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.3}$ Are the eigenvalues of these eigenvectors increasing or decreasing as you compute more eigenvectors? What do these eigenvalues say about the eigenvectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Principal Component Analysis to find the principal components in a dataset. \n",
    "Principal components can be seen as vectors along which most variance is found in the data. \n",
    "\n",
    "We will now create a matrix that reads data from `data/gaussian.txt`. The size of this matrix is $N × 2$, where $N$ is the number of points in the dataset. Then, we will use `plot_data()` to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_data(data, eigen_vectors = None):\n",
    "    \"\"\"\n",
    "    This function plots the data of the given `eigen_vectors` with a scatterplot of the matrix data. \n",
    "    If no eigenvectors are available, it just plots the data\n",
    "    :param data: the data\n",
    "    :param eigen_vectors: the eigenvectors\n",
    "    \"\"\"\n",
    "    # Plot the features as a scatterplot\n",
    "    x = [[el[0]] for el in data]\n",
    "    y = [[el[1]] for el in data]\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    if eigen_vectors:\n",
    "        # Plot the two PCA lines\n",
    "        for vector in eigen_vectors:\n",
    "            line = _set_line(vector)\n",
    "            plt.plot(line[0], line[1], 'red')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def _set_line(vector):\n",
    "    # Fixed number for the line size of this plot\n",
    "    line_size = 6\n",
    "\n",
    "    # Set the coordinates for the PCA lines\n",
    "    axis = np.zeros((2, 2))\n",
    "    axis[0][0] = vector[0] * line_size\n",
    "    axis[1][0] = vector[1] * line_size\n",
    "    axis[0][1] = vector[0] * -line_size\n",
    "    axis[1][1] = vector[1] * -line_size\n",
    "    return axis\n",
    "\n",
    "data = read_data(\"data/gaussian.txt\")\n",
    "plt.title('The 2D data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plot_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.1}$\n",
    "Can you predict the direction of the first principal component by looking at the plotted data? What about the second principal component?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2.1}$\n",
    "The principal components of a dataset can also be seen as the eigenvectors of the covariance\n",
    "matrix. Compute the covariance matrix of the Gaussian dataset as follows:  \n",
    "  \n",
    "$cov(X) = \\dfrac{1}{N}(X - \\bar{x})^\\intercal(X - \\bar{x})$\n",
    "\n",
    "where $\\bar{x}$ is a vector containing the mean of each feature. Next, compute the eigenvectors of this covariance matrix and plot the vectors accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(data):\n",
    "    \"\"\"\n",
    "    This function computes the computes the covariance matrix of a given `data`.\n",
    "    :param data: the starting data\n",
    "    :return: the covariance matrix \n",
    "    \"\"\"\n",
    "    # START ANSWER\n",
    "    # END ANSWER\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check your implementation with the numpy built-in cov function. \n",
    "data = read_data(\"data/gaussian.txt\")\n",
    "your_cov_matrix = covariance(data)\n",
    "np_cov_matrix = np.cov(np.transpose(data))\n",
    "\n",
    "# Your value might differ slightly as the numpy built-in cov function is a bit more precise than our function :).\n",
    "err_msg = \"Your covariance matrix is allowed to differ from the numpy matrix, but no more than ~+-0.025\"\n",
    "np.testing.assert_allclose(np_cov_matrix, your_cov_matrix, atol=0.025, err_msg=err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use power iteration method to compute the eigenvectors using your covariance matrix and plot the resulting vectors.\n",
    "# The plot should contain both the dataset (like in the above plot) and the eigenvectors.\n",
    "\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.2}$ Which of these eigenvectors captures the most variance? Was this the first principal component or the second?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next exercise, you will need to use the `create_image` function presented below.  \n",
    "__Note:__ If you have trouble installing PIL, try ```pip install Pillow```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "def create_image(fv):\n",
    "    \"\"\"\n",
    "    This function creates a grey image based on the given feature vector `fv`.\n",
    "    :param fv: the feature vector\n",
    "    :param title: the title of the image\n",
    "    \"\"\"\n",
    "    width = height = int(math.sqrt(len(fv)))\n",
    "\n",
    "    # Filter label and threshold from data\n",
    "    img = Image.new('L', (width, height), \"black\")\n",
    "    pixels = img.load()\n",
    "    min_v = min(fv)\n",
    "    max_v = max(fv)\n",
    "\n",
    "    # Iterate over each pixel and set p value\n",
    "    j = 0\n",
    "    for (idx, p) in enumerate(fv):\n",
    "        i = idx % width\n",
    "        pixel = int(((p - min_v) / (max_v - min_v) * 255))\n",
    "        pixels[i, j] = pixel\n",
    "        if i == (width - 1):\n",
    "            j += 1\n",
    "\n",
    "    # Resize image to make it better visible\n",
    "    img = img.resize((256, 256), Image.ANTIALIAS)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2.2}$ Create a matrix object and let it read in `data/faces.txt`. This matrix is of size $N × D$, where $N$ is the number of images in the dataset and $D$ is the number of pixels per image (in this case $32 × 32$, so $D = 1024$). Similar as before, compute the covariance matrix. Use this covariance matrix to compute the first 10 principal components and visualize them using the given `create_image()` function.    \n",
    "__Note:__ `create_image()` only accepts rows of the dataset (`lists`), use the `transpose` function of `numpy` to convert columns to rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display # To display images, usage: display(image)\n",
    "\n",
    "data = read_data(\"data/faces.txt\")\n",
    "\n",
    "# Display image of the mean\n",
    "mean = np.mean(data, axis=0)\n",
    "print(\"Mean image\")\n",
    "mean_image = create_image(mean)\n",
    "display(mean_image)\n",
    "\n",
    "# Now plot the image of each of the eigenvectors of the given dataset\n",
    "# START ANSWER\n",
    "# END ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.3}$ What do these principal components mean in terms of faces?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{2.3 (Optional)}$ Come back to the first part of the assignment (k-means clustering) and import your version of PCA to extract the features from our dataset. Do you see any differences in the results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
