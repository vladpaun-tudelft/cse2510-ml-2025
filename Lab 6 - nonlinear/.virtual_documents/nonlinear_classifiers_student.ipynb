














import numpy as np

data = np.load('data/heart_disease.npy')
data.shape





sorted_data = np.sort(data, axis=0)
frequencies = (sorted_data[1:,:] != sorted_data[:-1,:]).sum(axis=0) + 1
frequencies





import matplotlib.pyplot as plt

plt.hist(data[:, 13], np.arange(0, 4 + 1.5) - 0.5)
plt.title('The distribution of labels in the dataset')
plt.ylabel('Count')
plt.xlabel('Label')
plt.grid()
plt.show()








# Separate the array into features and labels
x = data[:, :13]
y = data[:, 13]

# Transform classes to booleans
# y = (y == 0), Numpy will repeat this equality check for each entry in the array
# and return an array of booleans.
y = y == np.zeros(len(y))

def split_dataset(x, y, random_state):
    # Split data into train and validation
    from sklearn.model_selection import train_test_split
    
    # For this assignment, we state the random_state variable.
    # This variable will be used as the seed for the random number generation so that the split is deterministic.
    # Therefore, all exercises will give the same results every run.
    x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=0.3, random_state=random_state) 

    # Separate features into discrete and numeric arrays. 
    # You can verify that the split (with a boundary of 5) is correct by looking at the data documentation.
    x_train_discrete = x_train[:, np.where(frequencies < 5)[0]].astype(int)
    x_train_numeric = x_train[:, np.where(frequencies > 5)[0]]
    x_validation_discrete = x_validation[:, np.where(frequencies < 5)[0]].astype(int)
    x_validation_numeric = x_validation[:, np.where(frequencies > 5)[0]]
    
    return x_train_discrete, x_train_numeric, x_validation_discrete, x_validation_numeric, y_train, y_validation

x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, 42)





# START ANSWER
print(f"x_train_disc: {x_train_disc[:10, :]}")
print(f"x_train_num: {x_train_num[:10, :]}")
print(f"x_validation_disc: {x_validation_disc[:10, :]}")
print(f"x_validation_num: {x_validation_num[:10, :]}")
print(f"y_train: {y_train[:10]}")
print(f"y_validation: {y_validation[:10]}")
# END ANSWER

assert y_train.shape[0] == x_train_disc.shape[0]
assert y_train.shape[0] == x_train_num.shape[0]
assert y_validation.shape[0] == x_validation_disc.shape[0]
assert y_validation.shape[0] == x_validation_num.shape[0]

assert x_train_disc.shape[1] == 8








import math

def ratio(labels):
    if len(labels) == 0:
        return 0
    # START ANSWER
    return np.mean(labels)
    # END ANSWER
    
print('Ratio for train set:', ratio(y_train))
print('Ratio for validation set:', ratio(y_validation))

# Verify the correctness of the ratio function
assert np.isclose(ratio(y_train), 0.53110)    
assert np.isclose(ratio(y_validation), 0.54444)    





def entropy_sub(p):
    """
    Returns the value for p * log_2(p)
    """
    # START ANSWER
    return 0 if p == 0 else p * np.log2(p)
    # END ANSWER 
    
def entropy(labels):
    """
    Returns the entropy of an array of labels, computed using equation (3.1.b)
    """
    # START ANSWER
    p = ratio(labels)
    return -entropy_sub(p) - entropy_sub(1 - p)
    # END ANSWER 

print('Entropy for train set:', entropy(y_train))
print('Entropy for validation set:', entropy(y_validation))

# Verify the correctness of the entropy function
assert np.isclose(entropy(y_train), 0.9972)    
assert np.isclose(entropy(y_validation), 0.9943)    





def split_entropy(label_lists, N):
    information = 0
    for label_list in label_lists:
        # START ANSWER
        information += (label_list.shape[0] / N ) * entropy(label_list)
        # END ANSWER
    return information

# Verify the correctness of the split_entropy function
labels = np.array([0, 0, 0, 0, 1, 1, 1, 1])
N = len(labels)
print('Entropy of the data before splitting:', entropy(labels))

# Worst case split
labels_list = np.array([[0, 0, 1, 1], [0, 0, 1, 1]])
print('Worst case:', split_entropy(labels_list, N))
assert np.isclose(split_entropy(labels_list, N), 1.0)    

# Better split
labels_list = np.array([[0, 0, 0, 1], [1, 1, 1, 0]])
print('Better:', split_entropy(labels_list, N))
assert np.isclose(split_entropy(labels_list, N), 0.81128)    

# Perfect split
labels_list = np.array([[0, 0, 0, 0], [1, 1, 1, 1]])
print('Optimal:', split_entropy(labels_list, N))
assert np.isclose(split_entropy(labels_list, N), 0.0)    





def information_gain(labels, indices): 
    labels_list = []
    for index_list in indices:
        labels_list.append(labels[index_list])
    # START ANSWER
    return entropy(labels) - split_entropy(labels_list, labels.shape[0])
    # END ANSWER
    
labels = np.array([0, 0, 0, 0, 1, 1, 1, 1])
labels_list = np.array([[0, 0, 0, 1], [1, 1, 1, 0]])

# Now we create `indices` that correspond to the indices of the split (compare them with the two lists above). 
# You will have to write code that creates such a list later.
indices = np.array([[0, 1, 2, 4], [5, 6, 7, 3]])

# Verify the correctness of the information_gain function
assert np.isclose(information_gain(labels, indices), 0.18872)    





from collections import defaultdict

class DecisionTree(object):
    def __init__(self, data_discrete, data_numeric, labels, tree_type=0, thres=0.1):
        """ Creates a Decision Tree, based on the following arguments:
                data_discrete - A 2D array of ints, each row containing the discrete features for a patient.
                data_numeric - A 2D array of floats, each row containing the numeric features for a patient.
                labels - An array of boolean class labels, each corresponding to a
                        DataRow instance of a patient at the same index. 
                tree_type - 0: create the Tree with the highest IG every node 
                            1: create DiscreteTrees only
                            2: create NumericTrees only
                thres - The cutoff value for IG, to stop splitting the tree.
                        Below this value the node becomes a leaf node and no
                        further splits are made.
            N.B. This function has already been provided and does not need to be modified."""
        # Store the basic attributes for any DecisionTree
        self.data_discrete = data_discrete
        self.data_numeric = data_numeric
        self.labels = labels
        self.tree_type = tree_type
        self.thres = thres
        
        # Compute the current ratio of labels and assign this node the most common label
        self.ratio = ratio(self.labels)
        # This will assign a boolean value to self.label, as `self.ratio >= 0.5` is a boolean statement
        self.label = self.ratio >= 0.5
        
        if self.tree_type == 1:
            # Convert this DecisionTree to a DiscreteTree and perform the split
            discr_tree = DiscreteTree(self)
            self.convert_tree(discr_tree)
        elif self.tree_type == 2:
            # Convert this DecisionTree to a NumericTree and perform the split
            numer_tree = NumericTree(self)
            self.convert_tree(numer_tree)
        else:
            # If no specific type has been given (tree_type: 0), we determine which type is best
            # by computing both options and comparing the IG.
            # Create a DiscreteTree and NumericTree, passing all the stored attributes
            # as an argument, and compute the best possible split for each
            discr_tree = DiscreteTree(self)
            numer_tree = NumericTree(self)
            
            # Based on the results of the split computations, replace this generic
            # DecisionTree node with either a DiscreteTree or a NumericTree node
            if discr_tree.info_gain > numer_tree.info_gain:
                self.convert_tree(discr_tree)
            else:
                self.convert_tree(numer_tree)
        
        # Create an empty dictionary to contain the (possible) branches from this node,
        # where the values should be new DecisionTree nodes, or None if not present
        self.branches = defaultdict(lambda: None)
        
        # Check if this split produced a high enough IG to actually create
        # the resulting branches with new split nodes below it,
        # else no split is carried out and the original node is a leaf node
        self.leaf = self.info_gain < self.thres
        if not self.leaf:
            self.create_subtrees()
    
    def store_split_values(self, feat_index, feat_values, indices, info_gain):
        """ Stores the values of the passed parameters as object attributes. Is intended
            to store the results of a split computation for either a DiscreteTree or a
            NumericTree. The stored attributes are:
                feat_index - The index of the feature on which the split was
                    based.
                feat_values - A list of the possible values that this split feature can
                    take, each corresponding to a different branch in the DecisionTree
                indices - A list of index lists, with each list containing the indices
                    defining a subset of the current data and label attributes, as
                    computed by the split. The order of these subsets should match the
                    order of the corresponding feat_values used to define the branches
                    of the split.
                info_gain - IG computed for this split
            N.B. This function has already been provided and does not need to be modified."""
        self.feat_index = feat_index
        self.feat_values = feat_values
        self.indices = indices
        self.info_gain = info_gain
    
    def convert_tree(self, new_tree):
        """ Converts this object to the tree passed as the new_tree parameter.
            All attributes from the new_tree are transfered.
                new_tree - Either a DiscreteTree or a NumericTree instance, to which
                            this object is converted
            N.B. This function has already been provided and does not need to be modified."""
        self.__class__ = new_tree.__class__
        self.__dict__ = new_tree.__dict__
    
    def create_subtrees(self):
        """ Creates the different subsets of the current data and labels, and makes a
            a new DecisionTree node for each such subset, based on the indices attribute
            stored after the computed split. These new DecisionTrees are stored in the 
            branches attribute, a dictionary mapping the value of a variable from the
            split to the new DecisionTree created by selecting that value for the split."""
        for i, key in enumerate(self.feat_values):
            subset_discrete = self.data_discrete[self.indices[i]]
            subset_numeric = self.data_numeric[self.indices[i]]
            subset_labels = self.labels[self.indices[i]]
            subtree = DecisionTree(subset_discrete, subset_numeric, subset_labels, tree_type=self.tree_type, thres=self.thres)
            self.branches[key] = subtree
        
    def classify(self, row_discrete, row_numeric):
        """ Traverses the DecisionTree based on the values stored in the given row and
            returns the most common label in the resulting leaf node.
                row - The index of the row being classified"""
        # Option 1: node is a leaf
        if self.leaf:
            return self.label
        
        subtree = self.get_subtree(row_discrete, row_numeric)
        
        # Option 2: no valid subtree
        if subtree is None:
            return self.label
        
        # Option 3: there is a valid subtree
        return subtree.classify(row_discrete, row_numeric)
        
    def split(self):
        """ Must be implemented by the subclass based on the specific type of split performed.
            The function here is only to ensure it is implemented, and should not be modified."""
        raise NotImplementedError
    
    def get_subtree(self, instance):
        """ Must be implemented by the subclass based on the specific type of split performed.
            The function here is only to ensure it is implemented, and should not be modified."""
        raise NotImplementedError





def create_indices_list(column):
    """ Creates the indices list, containing for each possible value of the current feature, 
        the indices of corresponding rows (e.g. [[0, 2], [1, 3], ...] where the current
        feature is 0 in rows 0 and 2).
        Returns the list of indices for all feature values and as second output a list of all possible feature values.
            column - The column of one feature from the data."""
    # START ANSWER
    feat_values = np.unique(column)
    indices_list = []
    for val in feat_values:
        indices_list.append(np.where(column == val)[0])
    return indices_list, feat_values
    # END ANSWER
    
vals = np.array([0, 0, 1, 0, 1, 1, 2, 2])
indices_list, feat_values = create_indices_list(vals)

# Verify the correctness of the create_indices_list function
assert ([[*i] for i in indices_list] == [[0, 1, 3], [2, 4, 5], [6, 7]])
assert np.array_equal(feat_values, [0, 1, 2])


class DiscreteTree(DecisionTree):
    def __init__(self, dtree):
        """ Takes a DecisionTree as initialization parameter and copies all its
            attributes. Then calls the split() function to determine the optimal
            discrete variable to split this subset of the data on.
                dtree - The DecisionTree instance whose attributes are copied to this
                        DiscreteTree instance.
            N.B. This function has already been provided and does not need to be
            modified."""
        self.__dict__ = dtree.__dict__.copy()
        self.split()

    def split(self):
        """ Determines the best discrete variable to split the current dataset on,
            based on the IG resulting from the split. For this best split variable, the
            function stores several resulting attributes from the split, using the
            store_split_values function. See the documentation of store_split_values
            for an overview of what should be stored."""
        max_feat = None
        max_feat_values = None
        max_split = None
        max_ig = 0
        
        for feat in range(self.data_discrete.shape[1]):
            # 1. Call create_indices_list() for the feature column.
            # 2. Compute the IG of the split
            # 3. If IG > max IG, update max values
            
            # START ANSWER
            indices_list, feat_values = create_indices_list(self.data_discrete[:, feat])
            ig = information_gain(self.labels, indices_list)
            if ig > max_ig:
                max_ig = ig
                max_feat_values = feat_values
                max_feat = feat
                max_split = indices_list
            # END ANSWER
            
        self.store_split_values(max_feat, max_feat_values, max_split, max_ig)
            
    def get_subtree(self, row_discrete, row_numeric):
        """ Returns the subtree one branch down.
            Returns None if the value was not present at the split.
                row_discrete - array of the discrete values
                row_numeric - array of the numeric values"""
        value = row_discrete[self.feat_index]
        return self.branches.get(value, None)











def validate(decision_tree, data_discrete, data_numeric, labels):
    """ Classifies all patient records and compares the outcome to 
        the provided labels. Returns the percentage of elements that was classified
        correctly.
            data_discrete - A 2D array of ints, each row containing the discrete features for a patient.
            data_numeric - A 2D array of floats, each row containing the numeric features for a patient.
            labels - List of boolean labels each belonging to a patient record"""
    # START ANSWER
    correct = 0
    n = len(labels)
    for i in range(n):
        pred = decision_tree.classify(data_discrete[i], data_numeric[i])
        if pred == labels[i]:
            correct += 1
    return correct / n if n else 0.0
    # END ANSWER





# Create a DecisionTree called 'trained_decision_tree' with tree_type 1 using the training data
# START ANSWER
trained_decision_tree = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=1)
# END ANSWER

# Verify the correctness of the validate function
result = validate(trained_decision_tree, x_validation_disc, x_validation_num, y_validation)
assert np.isclose(result, 0.765, rtol=0.02)  





def find_best_split(data, labels):
    max_feat = None
    max_split = None
    max_ig = 0
    max_boundary = None

    for feat in range(data.shape[1]):
        col = data[:, feat]

        for curr_boundary in col:
            # START ANSWER
            left_idx = np.where(col < curr_boundary)[0]
            right_idx = np.where (col >= curr_boundary)[0]
            indices_list = [left_idx, right_idx]
            ig = information_gain(labels, indices_list)
            if ig > max_ig:
                max_ig = ig
                max_boundary = curr_boundary
                max_split = indices_list
                max_feat = feat
            #END ANSWER
            
    return max_feat, max_split, max_ig, max_boundary

max_feat, max_split, max_ig, max_boundary = find_best_split(x_train_num[:10,:], y_train[:10])

# Verify the correctness of the find_best_split function
assert ([[*i] for i in max_split] == [[1, 3, 5, 6, 7], [0, 2, 4, 8, 9]])
assert np.array_equal(feat_values, [0, 1, 2])
assert max_feat == 3
assert np.isclose(max_ig, 0.27807)  
assert max_boundary == 159.0  


class NumericTree(DecisionTree):
    def __init__(self, dtree):
        """ Takes a DecisionTree as initialization parameter and copies all its
            attributes. Then calls the split() function to determine the optimal
            numeric variable to split this subset of the data on.
                dtree - The DecisionTree instance whose attributes are copied to this
                        NumericTree instance.
            N.B. This function has already been provided and does not need to be modified."""
        self.__dict__ = dtree.__dict__.copy()
        self.split()

    def split(self):
        """ Determines the best boundary for any numeric variable to split the
            current dataset on, based on the IG resulting from the split. For this
            best split boundary, the function stores several resulting attributes
            from the split, using the store_split_values function. See the
            documentation of store_split_values for an overview of what should
            be stored. In addition, one more attribute is stored in the numeric
            case, namely the boundary value used for the split."""
        max_feat, max_split, max_ig, boundary = find_best_split(self.data_numeric, self.labels)
        self.boundary = boundary
        
        max_feat_values = [False, True]
        self.store_split_values(max_feat, max_feat_values, max_split, max_ig)
        
    def get_subtree(self, row_discrete, row_numeric):
        """ Returns the subtree one branch down.
                row_discrete - array of the discrete values
                row_numeric - array of the numeric values"""
        value = row_numeric[self.feat_index] >= self.boundary
        return self.branches.get(value, None)








def get_accuracy(threshold = 0.1, n_iterations = 50, standarization=False):
    hybrid_accuracy = 0
    discrete_accuracy = 0
    numeric_accuracy = 0
    
    for i in range(n_iterations):

        x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, i)
        
        # START ANSWER
        hybrid = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=0, thres=threshold)
        disc = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=1, thres=threshold)
        num = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=2, thres=threshold)
        
        hybrid_accuracy += validate(hybrid, x_validation_disc, x_validation_num, y_validation)
        discrete_accuracy += validate(disc, x_validation_disc, x_validation_num, y_validation)
        numeric_accuracy += validate(num, x_validation_disc, x_validation_num, y_validation)
        # END ANSWER

    hybrid_accuracy /= n_iterations
    discrete_accuracy /= n_iterations
    numeric_accuracy /= n_iterations
    
    return hybrid_accuracy, discrete_accuracy, numeric_accuracy

hybrid, discrete, numeric = get_accuracy(0.1, standarization=False)

print('Hybrid tree accuracy:', hybrid)
print('Discrete tree accuracy:', discrete)
print('Numeric tree accuracy:', numeric)

# Verify the correctness of the get_accuracy function
assert np.isclose(hybrid, 0.744, rtol=0.05)  
assert np.isclose(discrete, 0.779, rtol=0.05)  
assert np.isclose(numeric, 0.695, rtol=0.05)  





def get_accuracy_standardize(threshold = 0.1, n_iterations = 50):
    hybrid_accuracy = 0
    discrete_accuracy = 0
    numeric_accuracy = 0
    
    for i in range(n_iterations):
    
        x_train_disc, x_train_num, x_validation_disc, x_validation_num, y_train, y_validation = split_dataset(x, y, i)
    
        # START ANSWER
        mean = np.mean(x_train_num)
        std = np.std(x_train_num)
        x_train_num = (x_train_num - mean) / std 
        x_validation_num = (x_validation_num - mean) / std
        # END ANSWER
            
        # START ANSWER
        hybrid = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=0, thres=threshold)
        disc = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=1, thres=threshold)
        num = DecisionTree(x_train_disc, x_train_num, y_train, tree_type=2, thres=threshold)
        
        hybrid_accuracy += validate(hybrid, x_validation_disc, x_validation_num, y_validation)
        discrete_accuracy += validate(disc, x_validation_disc, x_validation_num, y_validation)
        numeric_accuracy += validate(num, x_validation_disc, x_validation_num, y_validation)
        # END ANSWER

    hybrid_accuracy /= n_iterations
    discrete_accuracy /= n_iterations
    numeric_accuracy /= n_iterations
    
    return hybrid_accuracy, discrete_accuracy, numeric_accuracy

hybrid, discrete, numeric = get_accuracy_standardize(0.1)

print('Hybrid tree accuracy:', hybrid)
print('Discrete tree accuracy:', discrete)
print('Numeric tree accuracy:', numeric)

# Verify the correctness of the get_accuracy function
assert np.isclose(hybrid, 0.744, rtol=0.05)  
assert np.isclose(discrete, 0.779, rtol=0.05)  
assert np.isclose(numeric, 0.695, rtol=0.05)





threshold_values = np.linspace(0.0001, 0.3, 25)

accuracies_hybrid = []
accuracies_discrete = []
accuracies_numeric = []

for threshold in threshold_values:
    hybrid, discrete, numeric = get_accuracy(threshold, n_iterations=10)
    accuracies_hybrid.append(hybrid)
    accuracies_discrete.append(discrete)
    accuracies_numeric.append(numeric)
    
_, axis = plt.subplots()
axis.plot(threshold_values, accuracies_hybrid, label = 'hybrid')
axis.plot(threshold_values, accuracies_discrete, label = 'discrete')
axis.plot(threshold_values, accuracies_numeric, label = 'numeric')

axis.legend()
axis.set_xlabel('Threshold')
axis.set_ylabel('Accuracy')
plt.title('DecisionTree accuracy for different threshold values')
plt.grid()
plt.show()












